{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2031bc29-e21a-4bb2-be5b-8caad587aaad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1.Headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7203cab5-4bab-4e50-baf3-95aa77f256b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Heading 1 \n",
    "## Heading 2 \n",
    "### Heading 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08c5d0f-3748-4540-8d9c-d04dcb77a948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2.Bold ,Italic , Bold n Italic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50143176-e69d-4330-8a60-ce02e175eb3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Use ** or __ for bold text.\n",
    "Use * or _ for italic text.\n",
    "Use *** or ___ for bold and italic text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84309ca-06e9-4852-9ec1-e3cb23d80828",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Hello Bold sdfsgdfgdfgdgdgdfgdfgfdgdgdfgdfdfddfddfgdfgfdgdfgdfgdfgdgfdgfg** \n",
    "\n",
    "*Hello Italic Hello Italic Hello Italic Hello ic Hello Italic Hello Italic* \n",
    "\n",
    "***Hello Bold and Italic Hello Bold and Italic Hello Bold and Italic Hello Bold***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21eb2659-a2e8-4019-b8d8-4614cf91573a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3.List\n",
    "3.3. Lists:\n",
    "\n",
    "Unordered lists use -, +, or *.\n",
    "Ordered lists use numbers followed by a period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "253ec45a-bc7c-4922-9eb0-c7cd04952c33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Item 1 - Item 2   \n",
    "         - Subitem 1   - Subitem 2  \n",
    "1. First item 2. Second item    \n",
    "         1. Subitem 1    2. Subitem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ed9dd22-7704-41b5-b94f-172d867a1cb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#4.Image and Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487e0d3c-26ae-401e-bcec-10ce37ba57f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Links and Images:\n",
    "\n",
    "Links use [link text](URL).\n",
    "Images use ![alt text](URL).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ce2e983-0b45-4ed8-9bc2-5f8d6c4a5bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "[Databricks](https://databricks.com)  \n",
    "![Databricks Logo](https://databricks.com/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c629e8f-7859-4cd0-b00c-cf5e39db6da9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#5. Code Blocks\n",
    "\n",
    "Inline code uses backticks: `code`.\n",
    "Code blocks use triple backticks or indentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f115fd-f098-4333-a8a5-a735b4adaa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`inline code`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6e5cd6-544e-406d-a2cf-33aa3b217a03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n"
     ]
    }
   ],
   "source": [
    "def foo(): \n",
    "       return \"bar\" \n",
    "\n",
    "res=foo()\n",
    "print(res)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "908b2e4e-ba35-40fc-ad41-6384b396c783",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#6. Blockquotes\n",
    "\n",
    "Use > for blockquotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6939c857-8901-4000-b28c-43f438fa62da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "> This is a blockquote.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78ba22c-4302-417d-84db-58ec65ace83c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#7. Horizontal Lines\n",
    "\n",
    "Use ---, ***, or ___ to create horizontal lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdfb60b-8985-462e-b0e2-285c7939db30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---  \n",
    "*** \n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2218d70-74e1-4d01-bcf9-8ebcd0dbb4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34254966-2cd6-4e34-85a3-e0b701f7f2c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 Tables:\n",
    "Create tables using pipes | and dashes -."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ac63b6-a1c3-44a9-8e98-1fb3ce056aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "| Header 1 | Header 2 | \n",
    "| -------- | -------- | \n",
    "| Row 1    | Data 1   | \n",
    "| Row 2    | Data 2   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f27aa14-1843-4ac0-a76a-612666b61d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.2 Checkboxes:\n",
    "Create checkboxes using - [ ] for unchecked and - [x] for checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab6d0a2-56bb-4e82-b9d1-184f6e22445a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- [ ] Task 1 \n",
    "- [x] Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffee74b-40fb-41f5-8bbf-fa19221b606d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.3 HTML:\n",
    "You can also include HTML for more complex formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4e0916-c196-4c71-a3e8-d4339e5db300",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>Bold text</b> and <i>italic text</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f329ab-46c4-44d3-a82f-b1b86b0f77dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using Markdown in Databricks Notebooks\n",
    "* Creating a Markdown Cell:\n",
    "* In a Databricks notebook, click the + button to add a new cell.\n",
    "* Select Markdown from the cell type dropdown menu.\n",
    "* Writing Markdown:\n",
    "* Enter your markdown content in the cell.\n",
    "* When you run the cell (Shift + Enter), it will render the markdown as formatted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "851c1da5-849d-46ca-a1a0-77024fa40055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Examples\n",
    "Example 1: Documenting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5a6f8e-f021-473c-a18d-33aff526c12d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data:\n+-----------+----------+--------+----------+\n|customer_id|  txn_date|txn_type|txn_amount|\n+-----------+----------+--------+----------+\n|        429|2020-01-21| deposit|        82|\n|        155|2020-01-10| deposit|       712|\n|        398|2020-01-01| deposit|       196|\n|        255|2020-01-14| deposit|       563|\n|        185|2020-01-29| deposit|       626|\n+-----------+----------+--------+----------+\nonly showing top 5 rows\n\nCleaned Data:\n+-----------+----------+--------+----------+\n|customer_id|  txn_date|txn_type|txn_amount|\n+-----------+----------+--------+----------+\n|        429|2020-01-21| deposit|        82|\n|        155|2020-01-10| deposit|       712|\n|        398|2020-01-01| deposit|       196|\n|        255|2020-01-14| deposit|       563|\n|        185|2020-01-29| deposit|       626|\n+-----------+----------+--------+----------+\nonly showing top 5 rows\n\nSummary Statistics:\n+-------+------------------+----------+------------------+\n|summary|       customer_id|  txn_type|        txn_amount|\n+-------+------------------+----------+------------------+\n|  count|              5868|      5868|              5868|\n|   mean|251.13122017723245|      null| 504.2106339468303|\n| stddev|140.93615936277794|      null|288.12240241883245|\n|    min|                 1|   deposit|                 0|\n|    max|               500|withdrawal|              1000|\n+-------+------------------+----------+------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2040264437557170>:37\u001B[0m\n",
       "\u001B[1;32m     33\u001B[0m summary\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Additional analysis examples:\u001B[39;00m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Calculate the correlation between two columns\u001B[39;00m\n",
       "\u001B[0;32m---> 37\u001B[0m correlation \u001B[38;5;241m=\u001B[39m data_clean\u001B[38;5;241m.\u001B[39mcorr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorrelation between column1 and column2: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcorrelation\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Group by a specific column and calculate the average of another column\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4507\u001B[0m, in \u001B[0;36mDataFrame.corr\u001B[0;34m(self, col1, col2, method)\u001B[0m\n",
       "\u001B[1;32m   4502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpearson\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   4503\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   4504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently only the calculation of the Pearson Correlation \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   4505\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoefficient is supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   4506\u001B[0m     )\n",
       "\u001B[0;32m-> 4507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `column1` cannot be resolved. Did you mean one of the following? [`customer_id`, `txn_date`, `txn_type`, `txn_amount`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2040264437557170>:37\u001B[0m\n\u001B[1;32m     33\u001B[0m summary\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Additional analysis examples:\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Calculate the correlation between two columns\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m correlation \u001B[38;5;241m=\u001B[39m data_clean\u001B[38;5;241m.\u001B[39mcorr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorrelation between column1 and column2: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcorrelation\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Group by a specific column and calculate the average of another column\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4507\u001B[0m, in \u001B[0;36mDataFrame.corr\u001B[0;34m(self, col1, col2, method)\u001B[0m\n\u001B[1;32m   4502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpearson\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   4503\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   4504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently only the calculation of the Pearson Correlation \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4505\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoefficient is supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4506\u001B[0m     )\n\u001B[0;32m-> 4507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `column1` cannot be resolved. Did you mean one of the following? [`customer_id`, `txn_date`, `txn_type`, `txn_amount`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `column1` cannot be resolved. Did you mean one of the following? [`customer_id`, `txn_date`, `txn_type`, `txn_amount`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Introduction\n",
    "# This notebook demonstrates data analysis using PySpark in Databricks.\n",
    "\n",
    "# Steps:\n",
    "# 1. Load the data\n",
    "# 2. Clean the data\n",
    "# 3. Analyze the data\n",
    "# 4. Visualize the results\n",
    "\n",
    "# Code Example\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = spark.read.csv(\"dbfs:/FileStore/Transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(\"Loaded Data:\")\n",
    "data.show(5)\n",
    "\n",
    "# Step 2: Clean the data\n",
    "# Remove rows with any missing values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(\"Cleaned Data:\")\n",
    "data_clean.show(5)\n",
    "\n",
    "# Step 3: Analyze the data\n",
    "# Get summary statistics of the cleaned data\n",
    "summary = data_clean.describe()\n",
    "\n",
    "# Show the summary\n",
    "print(\"Summary Statistics:\")\n",
    "summary.show()\n",
    "\n",
    "# Additional analysis examples:\n",
    "# Calculate the correlation between two columns\n",
    "correlation = data_clean.corr(\"column1\", \"column2\")\n",
    "print(f\"Correlation between column1 and column2: {correlation}\")\n",
    "\n",
    "# Group by a specific column and calculate the average of another column\n",
    "grouped_data = data_clean.groupBy(\"customer_id\").avg(\"column2\")\n",
    "print(\"Grouped Data by column1 with average of column2:\")\n",
    "grouped_data.show()\n",
    "\n",
    "# Step 4: Visualize the results\n",
    "# Visualizations in Databricks can be created using the display function\n",
    "\n",
    "# Convert to Pandas DataFrame for easier plotting if necessary\n",
    "pandas_df = data_clean.toPandas()\n",
    "\n",
    "# Example: Histogram of a specific column\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(pandas_df['customer_id'], bins=20)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of column_name')\n",
    "plt.show()\n",
    "\n",
    "# Example: Scatter plot between two columns\n",
    "plt.scatter(pandas_df['column1'], pandas_df['column2'])\n",
    "plt.xlabel('column1')\n",
    "plt.ylabel('column2')\n",
    "plt.title('Scatter Plot between column1 and column2')\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, use Databricks' built-in visualization tools\n",
    "display(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08826cb6-5fd1-4a44-a013-14a57ed3347e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example 2 Creating a Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d3344b-98e4-485d-88d9-7eea950e73ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Example 2: Creating a Report**\n",
    "```markdown\n",
    "# Sales Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ad99a1-8970-47f7-8d33-b11415b0a18f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "This report provides a summary of sales data for the last quarter.\n",
    "## Key Metrics\n",
    "- **Total Sales**: $1,000,000\n",
    "- **Average Sales per Customer**: $500\n",
    "## Sales by Region\n",
    "| Region    | Sales       |\n",
    "| --------- | ----------- |\n",
    "| North     | $300,000    |\n",
    "| South     | $200,000    |\n",
    "| East      | $250,000    |\n",
    "| West      | $250,000    |\n",
    "## Conclusion\n",
    "The sales performance in the North and East regions exceeded expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc0fba27-1728-4ee0-a1cd-360eac0ecfab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773018e6-4f8d-40cc-874f-d6c2a9b3337d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tips for Effective Markdown Usage\n",
    "* Organize Content: Use headings and subheadings to organize your content into sections.\n",
    "\n",
    "* Highlight Important Information: Use bold and italic text to highlight key points.\n",
    "\n",
    "* Use Lists for Steps and Items: Lists are great for steps in a process or enumerating items.\n",
    "\n",
    "* Include Code Examples: Show code snippets inline or in blocks for clarity.\n",
    "\n",
    "* Visuals: Use tables, images, and links to provide additional context and details"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataBricks Magic Commands Tricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
